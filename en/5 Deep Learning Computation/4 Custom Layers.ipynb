{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooked-suspension",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-wellington",
   "metadata": {},
   "source": [
    "## Layers without Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "russian-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-utility",
   "metadata": {},
   "source": [
    "We can now incorporate our layer as a component\n",
    "in constructing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thrown-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-andrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2596e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-support",
   "metadata": {},
   "source": [
    "## Layers with Parameters\n",
    "\n",
    "Now that we know how to define simple layers,\n",
    "let us move on to defining layers with parameters\n",
    "that can be adjusted through training.\n",
    "We can use built-in functions to create parameters, which\n",
    "provide some basic housekeeping functionality.\n",
    "In particular, they govern access, initialization,\n",
    "sharing, saving, and loading model parameters.\n",
    "This way, among other benefits, we will not need to write\n",
    "custom serialization routines for every custom layer.\n",
    "\n",
    "Now let us implement our own version of the  fully-connected layer.\n",
    "Recall that this layer requires two parameters,\n",
    "one to represent the weight and the other for the bias.\n",
    "In this implementation, we bake in the ReLU activation as a default.\n",
    "This layer requires to input arguments: `in_units` and `units`, which\n",
    "denote the number of inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "related-engineer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1300,  0.6243,  0.2260],\n",
       "        [-0.2050,  0.8555,  1.4208],\n",
       "        [-0.3892, -0.0839, -1.4606],\n",
       "        [-1.9100,  1.2330,  1.6510],\n",
       "        [ 1.7910, -0.9983, -1.2444]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = MyLinear(5, 3)\n",
    "dense.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-portugal",
   "metadata": {},
   "source": [
    "We can directly carry out forward propagation calculations using custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "narrative-telephone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9926, 0.0000],\n",
       "        [1.1913, 0.9176, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-entity",
   "metadata": {},
   "source": [
    "We can also construct models using custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satisfied-router",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8937],\n",
       "        [8.1392]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-reward",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can design custom layers via the basic layer class. This allows us to define flexible new layers that behave differently from any existing layers in the library.\n",
    "* Once defined, custom layers can be invoked in arbitrary contexts and architectures.\n",
    "* Layers can have local parameters, which can be created through built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-somerset",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Design a layer that takes an input and computes a tensor reduction,\n",
    "   i.e., it returns $y_k = \\sum_{i, j} W_{ijk} x_i x_j$.\n",
    "1. Design a layer that returns the leading half of the Fourier coefficients of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
