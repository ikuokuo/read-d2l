{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joined-exclusion",
   "metadata": {},
   "source": [
    "# File I/O\n",
    "\n",
    "It is time to learn how to load and store\n",
    "both individual weight vectors and entire models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-encyclopedia",
   "metadata": {},
   "source": [
    "## Loading and Saving Tensors\n",
    "\n",
    "For individual tensors, we can directly\n",
    "invoke the `load` and `save` functions\n",
    "to read and write them respectively.\n",
    "Both functions require that we supply a name,\n",
    "and `save` requires as input the variable to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "middle-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-world",
   "metadata": {},
   "source": [
    "We can now read the data from the stored file back into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-disco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-burlington",
   "metadata": {},
   "source": [
    "We can store a list of tensors and read them back into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "linear-optimization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-saint",
   "metadata": {},
   "source": [
    "We can even write and read a dictionary that maps\n",
    "from strings to tensors.\n",
    "This is convenient when we want\n",
    "to read or write all the weights in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "statistical-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-partnership",
   "metadata": {},
   "source": [
    "## Loading and Saving Model Parameters\n",
    "\n",
    "An important detail to note is that this\n",
    "saves model *parameters* and not the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-publication",
   "metadata": {},
   "source": [
    "Next, we store the parameters of the model as a file with the name \"mlp.params\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worth-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-membrane",
   "metadata": {},
   "source": [
    "To recover the model, we instantiate a clone\n",
    "of the original MLP model.\n",
    "Instead of randomly initializing the model parameters,\n",
    "we read the parameters stored in the file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "selected-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-yellow",
   "metadata": {},
   "source": [
    "Since both instances have the same model parameters,\n",
    "the computational result of the same input `X` should be the same.\n",
    "Let us verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "falling-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-steps",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* The `save` and `load` functions can be used to perform file I/O for tensor objects.\n",
    "* We can save and load the entire sets of parameters for a network via a parameter dictionary.\n",
    "* Saving the architecture has to be done in code rather than in parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-convertible",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?\n",
    "1. Assume that we want to reuse only parts of a network to be incorporated into a network of a different architecture. How would you go about using, say the first two layers from a previous network in a new network?\n",
    "1. How would you go about saving the network architecture and parameters? What restrictions would you impose on the architecture?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
