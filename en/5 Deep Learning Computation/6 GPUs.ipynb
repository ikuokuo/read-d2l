{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equivalent-fusion",
   "metadata": {},
   "source": [
    "# GPUs\n",
    "\n",
    "In this section, we begin to discuss how to harness\n",
    "this computational performance for your research.\n",
    "First by using single GPUs and at a later point,\n",
    "how to use multiple GPUs and multiple servers (with multiple GPUs).\n",
    "\n",
    "Specifically, we will discuss how\n",
    "to use a single NVIDIA GPU for calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "existing-stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 10 22:33:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   37C    P0    20W /  N/A |    347MiB /  5934MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2670      G   /usr/lib/xorg/Xorg                160MiB |\n",
      "|    0   N/A  N/A      2825      G   /usr/bin/gnome-shell               28MiB |\n",
      "|    0   N/A  N/A      3217      G   ...AAAAAAAA== --shared-files       38MiB |\n",
      "|    0   N/A  N/A      5991      G   ...gAAAAAAAAA --shared-files      117MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-economics",
   "metadata": {},
   "source": [
    "In PyTorch, every array has a device, we often refer it as a context.\n",
    "So far, by default, all variables\n",
    "and associated computation\n",
    "have been assigned to the CPU.\n",
    "Typically, other contexts might be various GPUs.\n",
    "Things can get even hairier when\n",
    "we deploy jobs across multiple servers.\n",
    "By assigning arrays to contexts intelligently,\n",
    "we can minimize the time spent\n",
    "transferring data between devices.\n",
    "For example, when training neural networks on a server with a GPU,\n",
    "we typically prefer for the model's parameters to live on the GPU.\n",
    "\n",
    "To run the programs in this section,\n",
    "you need at least two GPUs.\n",
    "Note that this might be extravagant for most desktop computers\n",
    "but it is easily available in the cloud, e.g.,\n",
    "by using the AWS EC2 multi-GPU instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-locator",
   "metadata": {},
   "source": [
    "## Computing Devices\n",
    "\n",
    "We can specify devices, such as CPUs and GPUs,\n",
    "for storage and calculation.\n",
    "By default, tensors are created in the main memory\n",
    "and then use the CPU to calculate it.\n",
    "\n",
    "In PyTorch, the CPU and GPU can be indicated by `torch.device('cpu')` and `torch.cuda.device('cuda')`.\n",
    "It should be noted that the `cpu` device\n",
    "means all physical CPUs and memory.\n",
    "This means that PyTorch's calculations\n",
    "will try to use all CPU cores.\n",
    "However, a `gpu` device only represents one card\n",
    "and the corresponding memory.\n",
    "If there are multiple GPUs, we use `torch.cuda.device(f'cuda:{i}')`\n",
    "to represent the $i^\\mathrm{th}$ GPU ($i$ starts from 0).\n",
    "Also, `gpu:0` and `gpu` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indie-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " <torch.cuda.device at 0x7f2891757070>,\n",
       " <torch.cuda.device at 0x7f289176e3a0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.cuda.device('cuda'), torch.cuda.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-electricity",
   "metadata": {},
   "source": [
    "We can query the number of available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "particular-logan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-workplace",
   "metadata": {},
   "source": [
    "Now we define two convenient functions that allow us\n",
    "to run code even if the requested GPUs do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "superior-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-traveler",
   "metadata": {},
   "source": [
    "## Tensors and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "postal-secretary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-management",
   "metadata": {},
   "source": [
    "It is important to note that whenever we want\n",
    "to operate on multiple terms,\n",
    "they need to be on the same device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-anime",
   "metadata": {},
   "source": [
    "### Storage on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "removed-design",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secure-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2383, 0.0414, 0.0146],\n",
       "        [0.0180, 0.5063, 0.2946]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-civilization",
   "metadata": {},
   "source": [
    "### Copying\n",
    "\n",
    "If we want to compute `X + Y`,\n",
    "we need to decide where to perform this operation.\n",
    "\n",
    "![](../img/copyto.svg) Copy data to perform an operation on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reduced-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[0.2383, 0.0414, 0.0146],\n",
      "        [0.0180, 0.5063, 0.2946]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "Z = Y.cuda(0)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ignored-convenience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2383, 1.0414, 1.0146],\n",
       "        [1.0180, 1.5063, 1.2946]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monthly-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.cuda(0) is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-relay",
   "metadata": {},
   "source": [
    "### Side Notes\n",
    "\n",
    "People use GPUs to do machine learning\n",
    "because they expect them to be fast.\n",
    "But transferring variables between devices is slow.\n",
    "So we want you to be 100% certain\n",
    "that you want to do something slow before we let you do it.\n",
    "If the deep learning framework just did the copy automatically\n",
    "without crashing then you might not realize\n",
    "that you had written some slow code.\n",
    "\n",
    "Also, transferring data between devices (CPU, GPUs, and other machines)\n",
    "is something that is much slower than computation.\n",
    "It also makes parallelization a lot more difficult,\n",
    "since we have to wait for data to be sent (or rather to be received)\n",
    "before we can proceed with more operations.\n",
    "This is why copy operations should be taken with great care.\n",
    "As a rule of thumb, many small operations\n",
    "are much worse than one big operation.\n",
    "Moreover, several operations at a time\n",
    "are much better than many single operations interspersed in the code\n",
    "unless you know what you are doing.\n",
    "This is the case since such operations can block if one device\n",
    "has to wait for the other before it can do something else.\n",
    "\n",
    "Last, when we print tensors or convert tensors to the NumPy format,\n",
    "if the data is not in the main memory,\n",
    "the framework will copy it to the main memory first,\n",
    "resulting in additional transmission overhead.\n",
    "Even worse, it is now subject to the dreaded global interpreter lock\n",
    "that makes everything wait for Python to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-string",
   "metadata": {},
   "source": [
    "## Neural Networks and GPUs\n",
    "\n",
    "Similarly, a neural network model can specify devices.\n",
    "The following code puts the model parameters on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composite-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-greensboro",
   "metadata": {},
   "source": [
    "When the input is a tensor on the GPU, the model will calculate the result on the same GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "understanding-smile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4371],\n",
       "        [-0.4371]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-parade",
   "metadata": {},
   "source": [
    "Let us confirm that the model parameters are stored on the same GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "emerging-bacon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-paraguay",
   "metadata": {},
   "source": [
    "In short, as long as all data and parameters are on the same device, we can learn models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-pencil",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can specify devices for storage and calculation, such as the CPU or GPU.\n",
    "  By default, data are created in the main memory\n",
    "  and then use the CPU for calculations.\n",
    "* The deep learning framework requires all input data for calculation\n",
    "  to be on the same device,\n",
    "  be it CPU or the same GPU.\n",
    "* You can lose significant performance by moving data without care.\n",
    "  A typical mistake is as follows: computing the loss\n",
    "  for every minibatch on the GPU and reporting it back\n",
    "  to the user on the command line (or logging it in a NumPy `ndarray`)\n",
    "  will trigger a global interpreter lock which stalls all GPUs.\n",
    "  It is much better to allocate memory\n",
    "  for logging inside the GPU and only move larger logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-wiring",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try a larger computation task, such as the multiplication of large matrices,\n",
    "   and see the difference in speed between the CPU and GPU.\n",
    "   What about a task with a small amount of calculations?\n",
    "1. How should we read and write model parameters on the GPU?\n",
    "1. Measure the time it takes to compute 1000\n",
    "   matrix-matrix multiplications of $100 \\times 100$ matrices\n",
    "   and log the Frobenius norm of the output matrix one result at a time\n",
    "   vs. keeping a log on the GPU and transferring only the final result.\n",
    "1. Measure how much time it takes to perform two matrix-matrix multiplications\n",
    "   on two GPUs at the same time vs. in sequence\n",
    "   on one GPU. Hint: you should see almost linear scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
